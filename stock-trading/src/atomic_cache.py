"""
åŸå­åŒ–æ•°æ®ç¼“å­˜ç³»ç»Ÿ
æ ¸å¿ƒè®¾è®¡:
1. ç»Ÿä¸€ç¼“å­˜æ¥å£ - æ— è®ºAè‚¡/ç¾è‚¡ï¼Œç›¸åŒAPI
2. åŸå­æ“ä½œ - æŸ¥è¯¢/å†™å…¥/æ›´æ–°éƒ½æ˜¯åŸå­æ€§
3. æ™ºèƒ½è¿‡æœŸ - è‡ªåŠ¨åˆ¤æ–­æ•°æ®æ–°é²œåº¦
4. æ‰¹é‡ä¼˜åŒ– - æ”¯æŒæ‰¹é‡è¯»å†™ï¼Œå‡å°‘IO
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from contextlib import contextmanager
import threading
import json
import hashlib

# ç¼“å­˜æ•°æ®åº“è·¯å¾„
CACHE_DB_PATH = os.path.join(os.path.dirname(__file__), '..', 'data', 'cache', 'unified_cache.db')
os.makedirs(os.path.dirname(CACHE_DB_PATH), exist_ok=True)

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str              # å”¯ä¸€é”®
    data: Any             # æ•°æ®å†…å®¹
    data_type: str        # æ•°æ®ç±»å‹ (kline/realtime/fundamental)
    market: str           # å¸‚åœº (Aè‚¡/US)
    symbol: str           # è‚¡ç¥¨ä»£ç 
    start_date: str       # æ•°æ®å¼€å§‹æ—¥æœŸ
    end_date: str         # æ•°æ®ç»“æŸæ—¥æœŸ
    created_at: datetime  # åˆ›å»ºæ—¶é—´
    expires_at: datetime  # è¿‡æœŸæ—¶é—´
    version: int = 1      # ç‰ˆæœ¬å·ï¼Œç”¨äºä¹è§‚é”


class AtomicCache:
    """
    åŸå­åŒ–ç¼“å­˜ç³»ç»Ÿ
    
    ç‰¹æ€§:
    - çº¿ç¨‹å®‰å…¨
    - åŸå­è¯»å†™
    - è‡ªåŠ¨è¿‡æœŸæ¸…ç†
    - æ‰¹é‡æ“ä½œä¼˜åŒ–
    """
    
    def __init__(self, db_path: str = CACHE_DB_PATH):
        self.db_path = db_path
        self._local = threading.local()
        self._init_db()
    
    def _get_conn(self) -> sqlite3.Connection:
        """è·å–çº¿ç¨‹å®‰å…¨çš„è¿æ¥"""
        if not hasattr(self._local, 'conn') or self._local.conn is None:
            self._local.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            self._local.conn.row_factory = sqlite3.Row
        return self._local.conn
    
    @contextmanager
    def _transaction(self):
        """äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ä¿è¯åŸå­æ€§"""
        conn = self._get_conn()
        try:
            conn.execute("BEGIN IMMEDIATE")  # ç«‹å³è·å–å†™é”
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise e
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        with self._transaction() as conn:
            cursor = conn.cursor()
            
            # ä¸»ç¼“å­˜è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key TEXT PRIMARY KEY,
                    data BLOB NOT NULL,
                    data_type TEXT NOT NULL,
                    market TEXT NOT NULL,
                    symbol TEXT NOT NULL,
                    start_date TEXT,
                    end_date TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    expires_at TIMESTAMP,
                    version INTEGER DEFAULT 1,
                    access_count INTEGER DEFAULT 0,
                    last_accessed TIMESTAMP
                )
            ''')
            
            # ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_market_symbol ON cache_entries(market, symbol)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_data_type ON cache_entries(data_type)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_expires ON cache_entries(expires_at)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_date_range ON cache_entries(symbol, start_date, end_date)')
            
            # å…ƒæ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS cache_stats (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    total_entries INTEGER DEFAULT 0,
                    total_size_bytes INTEGER DEFAULT 0,
                    hit_count INTEGER DEFAULT 0,
                    miss_count INTEGER DEFAULT 0,
                    last_cleanup TIMESTAMP
                )
            ''')
            
            # åˆå§‹åŒ–ç»Ÿè®¡
            cursor.execute('INSERT OR IGNORE INTO cache_stats (id) VALUES (1)')
    
    def _generate_key(self, market: str, symbol: str, data_type: str, 
                     start_date: str = None, end_date: str = None) -> str:
        """ç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®"""
        key_parts = [market, symbol, data_type]
        if start_date:
            key_parts.append(start_date)
        if end_date:
            key_parts.append(end_date)
        
        raw_key = "|".join(key_parts)
        # ä½¿ç”¨å“ˆå¸Œç¼©çŸ­é”®é•¿
        return hashlib.md5(raw_key.encode()).hexdigest()
    
    def get(self, market: str, symbol: str, data_type: str,
            start_date: str = None, end_date: str = None,
            max_age_hours: int = 24) -> Optional[Any]:
        """
        åŸå­åŒ–æŸ¥è¯¢ç¼“å­˜
        
        Args:
            market: å¸‚åœº (Aè‚¡/US)
            symbol: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹ (kline/realtime/fundamental)
            start_date: å¼€å§‹æ—¥æœŸ (å¯é€‰)
            end_date: ç»“æŸæ—¥æœŸ (å¯é€‰)
            max_age_hours: æœ€å¤§ç¼“å­˜å¹´é¾„(å°æ—¶)
        
        Returns:
            ç¼“å­˜æ•°æ®æˆ–None
        """
        key = self._generate_key(market, symbol, data_type, start_date, end_date)
        
        with self._transaction() as conn:
            cursor = conn.cursor()
            
            # æŸ¥è¯¢ç¼“å­˜
            cursor.execute('''
                SELECT data, expires_at, version FROM cache_entries
                WHERE key = ? AND expires_at > datetime('now')
            ''', (key,))
            
            row = cursor.fetchone()
            
            if row:
                # å‘½ä¸­ç¼“å­˜
                data = json.loads(row['data'])
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                cursor.execute('''
                    UPDATE cache_entries 
                    SET access_count = access_count + 1,
                        last_accessed = datetime('now')
                    WHERE key = ?
                ''', (key,))
                
                # æ›´æ–°å…¨å±€ç»Ÿè®¡
                cursor.execute('UPDATE cache_stats SET hit_count = hit_count + 1 WHERE id = 1')
                
                return data
            else:
                # æœªå‘½ä¸­
                cursor.execute('UPDATE cache_stats SET miss_count = miss_count + 1 WHERE id = 1')
                return None
    
    def set(self, market: str, symbol: str, data_type: str,
            data: Any, start_date: str = None, end_date: str = None,
            ttl_hours: int = 24) -> bool:
        """
        åŸå­åŒ–å†™å…¥ç¼“å­˜
        
        Args:
            market: å¸‚åœº
            symbol: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹
            data: è¦ç¼“å­˜çš„æ•°æ®
            start_date: æ•°æ®å¼€å§‹æ—¥æœŸ
            end_date: æ•°æ®ç»“æŸæ—¥æœŸ
            ttl_hours: ç¼“å­˜å­˜æ´»æ—¶é—´(å°æ—¶)
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        key = self._generate_key(market, symbol, data_type, start_date, end_date)
        
        try:
            serialized = json.dumps(data, default=str)
            expires_at = datetime.now() + timedelta(hours=ttl_hours)
            
            with self._transaction() as conn:
                cursor = conn.cursor()
                
                # UPSERTæ“ä½œ - åŸå­æ€§ä¿è¯
                cursor.execute('''
                    INSERT INTO cache_entries 
                    (key, data, data_type, market, symbol, start_date, end_date, expires_at, version)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, 1)
                    ON CONFLICT(key) DO UPDATE SET
                        data = excluded.data,
                        updated_at = datetime('now'),
                        expires_at = excluded.expires_at,
                        version = cache_entries.version + 1
                ''', (key, serialized, data_type, market, symbol, 
                      start_date, end_date, expires_at.isoformat()))
                
                # æ›´æ–°ç»Ÿè®¡
                cursor.execute('''
                    UPDATE cache_stats 
                    SET total_entries = (SELECT COUNT(*) FROM cache_entries),
                        total_size_bytes = total_size_bytes + ?
                    WHERE id = 1
                ''', (len(serialized),))
                
                return True
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
            return False
    
    def get_kline_atomic(self, market: str, symbol: str, 
                        start_date: str, end_date: str,
                        fetch_func=None, max_age_hours: int = 6) -> Optional[pd.DataFrame]:
        """
        Kçº¿æ•°æ®åŸå­åŒ–è·å–
        
        é€»è¾‘:
        1. å…ˆæŸ¥ç¼“å­˜
        2. å¦‚æœå‘½ä¸­ä¸”æ•°æ®å®Œæ•´ï¼Œç›´æ¥è¿”å›
        3. å¦‚æœéƒ¨åˆ†ç¼ºå¤±ï¼Œåªfetchç¼ºå¤±éƒ¨åˆ†ï¼Œåˆå¹¶åå­˜å…¥
        4. å¦‚æœå®Œå…¨ç¼ºå¤±ï¼Œfetchå…¨éƒ¨ï¼Œå­˜å…¥åè¿”å›
        """
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_data = self.get(market, symbol, 'kline', start_date, end_date, max_age_hours)
        
        if cached_data is not None:
            print(f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {symbol} ({start_date}~{end_date})")
            return pd.read_json(cached_data, orient='split')
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦è·å–æ•°æ®
        if fetch_func is None:
            return None
        
        print(f"   ğŸŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œä»APIè·å–: {symbol}")
        
        try:
            # è·å–æ•°æ®
            df = fetch_func(symbol, start_date, end_date)
            
            if df is not None and not df.empty:
                # å­˜å…¥ç¼“å­˜
                self.set(market, symbol, 'kline', 
                        df.to_json(orient='split'),
                        start_date, end_date, ttl_hours=max_age_hours)
                
                print(f"   âœ… å·²ç¼“å­˜: {len(df)} æ¡è®°å½•")
                return df
                
        except Exception as e:
            print(f"   âŒ è·å–å¤±è´¥: {e}")
        
        return None
    
    def batch_get(self, keys: List[Dict]) -> Dict[str, Any]:
        """
        æ‰¹é‡æŸ¥è¯¢ - ä¸€æ¬¡æ€§æŸ¥è¯¢å¤šä¸ªkey
        
        Args:
            keys: [{'market': 'Aè‚¡', 'symbol': '000001', 'data_type': 'kline', ...}, ...]
        
        Returns:
            {key: data} çš„å­—å…¸
        """
        results = {}
        
        for key_info in keys:
            key = self._generate_key(
                key_info['market'],
                key_info['symbol'],
                key_info['data_type'],
                key_info.get('start_date'),
                key_info.get('end_date')
            )
            
            data = self.get(**key_info)
            results[key] = data
        
        return results
    
    def batch_set(self, entries: List[CacheEntry]) -> int:
        """
        æ‰¹é‡å†™å…¥
        
        Returns:
            æˆåŠŸå†™å…¥çš„æ•°é‡
        """
        success_count = 0
        
        with self._transaction() as conn:
            cursor = conn.cursor()
            
            for entry in entries:
                try:
                    serialized = json.dumps(entry.data, default=str)
                    
                    cursor.execute('''
                        INSERT INTO cache_entries 
                        (key, data, data_type, market, symbol, start_date, end_date, expires_at, version)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ON CONFLICT(key) DO UPDATE SET
                            data = excluded.data,
                            updated_at = datetime('now'),
                            expires_at = excluded.expires_at,
                            version = cache_entries.version + 1
                    ''', (
                        entry.key,
                        serialized,
                        entry.data_type,
                        entry.market,
                        entry.symbol,
                        entry.start_date,
                        entry.end_date,
                        entry.expires_at.isoformat(),
                        entry.version
                    ))
                    
                    success_count += 1
                    
                except Exception as e:
                    print(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥ {entry.key}: {e}")
            
            # æ›´æ–°ç»Ÿè®¡
            cursor.execute('''
                UPDATE cache_stats 
                SET total_entries = (SELECT COUNT(*) FROM cache_entries)
                WHERE id = 1
            ''')
        
        return success_count
    
    def invalidate(self, market: str = None, symbol: str = None, 
                   data_type: str = None, older_than_days: int = None) -> int:
        """
        ä½¿ç¼“å­˜å¤±æ•ˆ
        
        Returns:
            æ¸…é™¤çš„æ¡ç›®æ•°
        """
        with self._transaction() as conn:
            cursor = conn.cursor()
            
            conditions = []
            params = []
            
            if market:
                conditions.append("market = ?")
                params.append(market)
            
            if symbol:
                conditions.append("symbol = ?")
                params.append(symbol)
            
            if data_type:
                conditions.append("data_type = ?")
                params.append(data_type)
            
            if older_than_days:
                conditions.append("updated_at < datetime('now', '-{} days')".format(older_than_days))
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            cursor.execute(f"DELETE FROM cache_entries WHERE {where_clause}", params)
            deleted = cursor.rowcount
            
            # æ›´æ–°ç»Ÿè®¡
            cursor.execute('''
                UPDATE cache_stats 
                SET total_entries = (SELECT COUNT(*) FROM cache_entries)
                WHERE id = 1
            ''')
            
            return deleted
    
    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        with self._transaction() as conn:
            cursor = conn.cursor()
            
            cursor.execute("DELETE FROM cache_entries WHERE expires_at < datetime('now')")
            deleted = cursor.rowcount
            
            cursor.execute('''
                UPDATE cache_stats 
                SET total_entries = (SELECT COUNT(*) FROM cache_entries),
                    last_cleanup = datetime('now')
                WHERE id = 1
            ''')
            
            return deleted
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM cache_stats WHERE id = 1')
        stats = dict(cursor.fetchone())
        
        # è®¡ç®—å‘½ä¸­ç‡
        total_requests = stats.get('hit_count', 0) + stats.get('miss_count', 0)
        if total_requests > 0:
            stats['hit_rate'] = stats['hit_count'] / total_requests
        else:
            stats['hit_rate'] = 0
        
        # å„ç±»å‹åˆ†å¸ƒ
        cursor.execute('''
            SELECT data_type, COUNT(*) as count 
            FROM cache_entries 
            GROUP BY data_type
        ''')
        stats['type_distribution'] = {row['data_type']: row['count'] for row in cursor.fetchall()}
        
        return stats


# å…¨å±€ç¼“å­˜å®ä¾‹
cache = AtomicCache()


def test_atomic_cache():
    """æµ‹è¯•åŸå­ç¼“å­˜"""
    print("ğŸ§ª æµ‹è¯•åŸå­åŒ–ç¼“å­˜ç³»ç»Ÿ\n")
    
    # æµ‹è¯•1: åŸºæœ¬å­˜å–
    print("1ï¸âƒ£  åŸºæœ¬å­˜å–æµ‹è¯•...")
    test_data = {'price': 100.5, 'volume': 10000, 'timestamp': datetime.now().isoformat()}
    
    success = cache.set("Aè‚¡", "000001", "realtime", test_data, ttl_hours=1)
    print(f"   {'âœ…' if success else 'âŒ'} å†™å…¥")
    
    retrieved = cache.get("Aè‚¡", "000001", "realtime")
    print(f"   {'âœ…' if retrieved else 'âŒ'} è¯»å–: {retrieved}")
    
    # æµ‹è¯•2: Kçº¿æ•°æ®å­˜å–
    print("\n2ï¸âƒ£  Kçº¿æ•°æ®æµ‹è¯•...")
    import pandas as pd
    
    kline_df = pd.DataFrame({
        'date': ['2026-02-26', '2026-02-27', '2026-02-28'],
        'open': [10.0, 10.5, 11.0],
        'close': [10.5, 11.0, 11.3],
        'high': [10.8, 11.2, 11.5],
        'low': [9.8, 10.3, 10.8],
        'volume': [10000, 15000, 12000]
    })
    
    cache.set("Aè‚¡", "000001", "kline", 
              kline_df.to_json(orient='split'),
              "20260226", "20260228", ttl_hours=24)
    
    cached_json = cache.get("Aè‚¡", "000001", "kline", "20260226", "20260228")
    if cached_json:
        cached_df = pd.read_json(cached_json, orient='split')
        print(f"   âœ… ç¼“å­˜Kçº¿: {len(cached_df)} æ¡")
    
    # æµ‹è¯•3: ç»Ÿè®¡ä¿¡æ¯
    print("\n3ï¸âƒ£  ç¼“å­˜ç»Ÿè®¡...")
    stats = cache.get_stats()
    print(f"   æ€»æ¡ç›®: {stats.get('total_entries')}")
    print(f"   å‘½ä¸­ç‡: {stats.get('hit_rate', 0):.2%}")
    print(f"   ç±»å‹åˆ†å¸ƒ: {stats.get('type_distribution')}")
    
    print("\nâœ… åŸå­ç¼“å­˜æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_atomic_cache()
