"""
æ‰¹é‡å›æµ‹å¼•æ“ - Q è„‘é‡åŒ–äº¤æ˜“ç³»ç»Ÿ
================================
æ”¯æŒå¤šç­–ç•¥å¹¶è¡Œå›æµ‹ï¼Œä¼˜åŒ–çš„æ€§èƒ½è®¾è®¡ï¼ŒA è‚¡ ETF ä¸“ç”¨

åŠŸèƒ½ï¼š
- å¤šçº¿ç¨‹/å¤šè¿›ç¨‹å¹¶è¡Œå›æµ‹
- å¢é‡å›æµ‹æ”¯æŒ
- ç»“æœç¼“å­˜
- å®æ—¶è¿›åº¦è¿½è¸ª
- å†…å­˜ä¼˜åŒ–

ä½œè€…ï¼šBacker-Agent (Q è„‘å›æµ‹æ¶æ„å¸ˆ)
åˆ›å»ºï¼š2026-03-01
"""

import sys
import os
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from dataclasses import dataclass, field, asdict
from pathlib import Path
import hashlib
import pickle
import numpy as np
import pandas as pd

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.backtest.engine import (
    BacktestEngine, Event, Bar, Order, Fill,
    EventType, OrderSide, OrderType,
    FixedSlippage, VolatilitySlippage,
    SquareRootImpact, LinearImpact
)
from src.backtest.performance import PerformanceAnalyzer, PerformanceMetrics, Trade

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class BacktestConfig:
    """å›æµ‹é…ç½®"""
    symbol: str
    name: str = ""
    start_date: str = "2024-01-01"
    end_date: str = "2026-12-31"
    initial_capital: float = 100000.0
    commission_rate: float = 0.0003  # ä¸‡åˆ†ä¹‹ä¸‰
    slippage_model: str = "fixed"  # fixed, volatility
    slippage_params: Dict = field(default_factory=lambda: {"slippage_per_share": 0.01})
    impact_model: str = "sqrt"  # sqrt, linear
    impact_params: Dict = field(default_factory=lambda: {"impact_factor": 0.1})
    freq: str = "1d"  # 1d (æ—¥çº§), 1m (åˆ†é’Ÿçº§)
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'BacktestConfig':
        return cls(**data)


@dataclass
class BacktestResult:
    """å›æµ‹ç»“æœ"""
    symbol: str
    name: str
    config: BacktestConfig
    status: str  # success, error, skipped
    metrics: Optional[PerformanceMetrics] = None
    trades: List[Trade] = field(default_factory=list)
    equity_curve: List[float] = field(default_factory=list)
    dates: List[str] = field(default_factory=list)
    error_message: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    cache_hit: bool = False
    
    @property
    def duration_seconds(self) -> float:
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆå¯åºåˆ—åŒ–ï¼‰"""
        result = {
            'symbol': self.symbol,
            'name': self.name,
            'config': self.config.to_dict(),
            'status': self.status,
            'metrics': None,
            'trades': [],
            'equity_curve': self.equity_curve,
            'dates': self.dates,
            'error_message': self.error_message,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'cache_hit': self.cache_hit,
            'duration_seconds': self.duration_seconds
        }
        
        if self.metrics:
            result['metrics'] = asdict(self.metrics)
        
        for trade in self.trades:
            result['trades'].append({
                'symbol': trade.symbol,
                'entry_date': trade.entry_date.isoformat() if trade.entry_date else None,
                'exit_date': trade.exit_date.isoformat() if trade.exit_date else None,
                'entry_price': trade.entry_price,
                'exit_price': trade.exit_price,
                'quantity': trade.quantity,
                'side': trade.side,
                'pnl': trade.pnl,
                'pnl_pct': trade.pnl_pct,
                'commission': trade.commission,
                'slippage': trade.slippage,
                'impact_cost': trade.impact_cost,
                'is_open': trade.is_open
            })
        
        return result
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'BacktestResult':
        """ä»å­—å…¸åˆ›å»º"""
        config = BacktestConfig.from_dict(data['config'])
        
        result = cls(
            symbol=data['symbol'],
            name=data['name'],
            config=config,
            status=data['status'],
            equity_curve=data.get('equity_curve', []),
            dates=data.get('dates', []),
            error_message=data.get('error_message'),
            cache_hit=data.get('cache_hit', False)
        )
        
        if data.get('start_time'):
            result.start_time = datetime.fromisoformat(data['start_time'])
        if data.get('end_time'):
            result.end_time = datetime.fromisoformat(data['end_time'])
        
        if data.get('metrics'):
            metrics_dict = data['metrics']
            result.metrics = PerformanceMetrics(**metrics_dict)
        
        for trade_dict in data.get('trades', []):
            trade = Trade(
                symbol=trade_dict['symbol'],
                entry_date=datetime.fromisoformat(trade_dict['entry_date']) if trade_dict['entry_date'] else None,
                exit_date=datetime.fromisoformat(trade_dict['exit_date']) if trade_dict['exit_date'] else None,
                entry_price=trade_dict['entry_price'],
                exit_price=trade_dict['exit_price'],
                quantity=trade_dict['quantity'],
                side=trade_dict['side'],
                pnl=trade_dict['pnl'],
                pnl_pct=trade_dict['pnl_pct'],
                commission=trade_dict['commission'],
                slippage=trade_dict['slippage'],
                impact_cost=trade_dict['impact_cost']
            )
            result.trades.append(trade)
        
        return result


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "cache/backtest"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_cache_key(self, config: BacktestConfig, strategy_hash: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{config.symbol}_{config.start_date}_{config.end_date}_{strategy_hash}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_cache_path(self, key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{key}.pkl"
    
    def get(self, key: str) -> Optional[BacktestResult]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        cache_path = self._get_cache_path(key)
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥ {key}: {e}")
        return None
    
    def set(self, key: str, result: BacktestResult):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(key)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(result, f)
            logger.debug(f"ç¼“å­˜å·²ä¿å­˜ {key}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥ {key}: {e}")
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        for f in self.cache_dir.glob("*.pkl"):
            f.unlink()
        logger.info("ç¼“å­˜å·²æ¸…ç©º")


class BatchBacktester:
    """æ‰¹é‡å›æµ‹å¼•æ“"""
    
    def __init__(
        self,
        data_source: Callable[[str, str, str], pd.DataFrame],
        strategy_func: Callable[[pd.DataFrame, Dict], Tuple[pd.Series, pd.Series]],
        cache_enabled: bool = True,
        cache_dir: str = "cache/backtest",
        max_workers: int = 4
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å›æµ‹å¼•æ“
        
        Args:
            data_source: æ•°æ®æºå‡½æ•° (symbol, start_date, end_date) -> DataFrame
            strategy_func: ç­–ç•¥å‡½æ•° (df, params) -> (buy_signals, sell_signals)
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_dir: ç¼“å­˜ç›®å½•
            max_workers: æœ€å¤§å¹¶å‘æ•°
        """
        self.data_source = data_source
        self.strategy_func = strategy_func
        self.cache_manager = CacheManager(cache_dir) if cache_enabled else None
        self.max_workers = max_workers
        self.results: List[BacktestResult] = []
        self.progress_callback: Optional[Callable[[int, int, BacktestResult], None]] = None
    
    def set_progress_callback(self, callback: Callable[[int, int, BacktestResult], None]):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
    
    def _run_single_backtest(self, config: BacktestConfig, strategy_params: Dict = None) -> BacktestResult:
        """æ‰§è¡Œå•ä¸ªå›æµ‹"""
        start_time = datetime.now()
        
        if strategy_params is None:
            strategy_params = {}
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_manager:
            strategy_hash = hashlib.md5(json.dumps(strategy_params, sort_keys=True).encode()).hexdigest()
            cache_key = self.cache_manager._get_cache_key(config, strategy_hash)
            cached_result = self.cache_manager.get(cache_key)
            if cached_result:
                cached_result.cache_hit = True
                cached_result.end_time = datetime.now()
                logger.info(f"âœ“ {config.symbol}: ç¼“å­˜å‘½ä¸­")
                return cached_result
        
        try:
            # è·å–æ•°æ®
            df = self.data_source(config.symbol, config.start_date, config.end_date)
            
            if df is None or len(df) < 60:
                result = BacktestResult(
                    symbol=config.symbol,
                    name=config.name,
                    config=config,
                    status='skipped',
                    error_message='æ•°æ®ä¸è¶³',
                    start_time=start_time,
                    end_time=datetime.now()
                )
                return result
            
            # ç”Ÿæˆä¿¡å·
            buy_signals, sell_signals = self.strategy_func(df, strategy_params)
            
            # åˆ›å»ºå›æµ‹å¼•æ“
            engine = BacktestEngine(
                initial_capital=config.initial_capital,
                commission_rate=config.commission_rate
            )
            
            # è®¾ç½®æ»‘ç‚¹æ¨¡å‹
            if config.slippage_model == "fixed":
                engine.set_slippage_model(FixedSlippage(**config.slippage_params))
            elif config.slippage_model == "volatility":
                engine.set_slippage_model(VolatilitySlippage(**config.slippage_params))
            
            # è®¾ç½®å†²å‡»æˆæœ¬æ¨¡å‹
            if config.impact_model == "sqrt":
                engine.set_impact_model(SquareRootImpact(**config.impact_params))
            elif config.impact_model == "linear":
                engine.set_impact_model(LinearImpact(**config.impact_params))
            
            # è¿è¡Œå›æµ‹
            engine.run_backtest(df, buy_signals, sell_signals, freq=config.freq)
            
            # è·å–ç»“æœ
            metrics = engine.get_metrics()
            trades = engine.get_trades()
            equity_curve = engine.get_equity_curve()
            dates = [d.strftime('%Y-%m-%d') for d in engine.get_dates()]
            
            result = BacktestResult(
                symbol=config.symbol,
                name=config.name,
                config=config,
                status='success',
                metrics=metrics,
                trades=trades,
                equity_curve=equity_curve,
                dates=dates,
                start_time=start_time,
                end_time=datetime.now()
            )
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.cache_manager:
                self.cache_manager.set(cache_key, result)
            
            logger.info(f"âœ“ {config.symbol}: æ”¶ç›Š {metrics.total_return:+.2f}% (Sharpe: {metrics.sharpe_ratio:.2f})")
            
        except Exception as e:
            logger.error(f"âœ— {config.symbol}: {e}")
            result = BacktestResult(
                symbol=config.symbol,
                name=config.name,
                config=config,
                status='error',
                error_message=str(e),
                start_time=start_time,
                end_time=datetime.now()
            )
        
        return result
    
    def run_batch(
        self,
        configs: List[BacktestConfig],
        strategy_params: Dict = None,
        use_multiprocessing: bool = False
    ) -> List[BacktestResult]:
        """
        æ‰¹é‡å›æµ‹
        
        Args:
            configs: å›æµ‹é…ç½®åˆ—è¡¨
            strategy_params: ç­–ç•¥å‚æ•°
            use_multiprocessing: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹ (é€‚åˆ CPU å¯†é›†å‹)
        
        Returns:
            å›æµ‹ç»“æœåˆ—è¡¨
        """
        if strategy_params is None:
            strategy_params = {}
        
        total = len(configs)
        logger.info(f"ğŸš€ å¯åŠ¨æ‰¹é‡å›æµ‹ï¼š{total} ä¸ªæ ‡çš„")
        logger.info(f"å¹¶å‘æ•°ï¼š{self.max_workers}")
        
        self.results = []
        start_time = time.time()
        
        if use_multiprocessing and self.max_workers > 1:
            # å¤šè¿›ç¨‹æ¨¡å¼ (é€‚åˆ CPU å¯†é›†å‹)
            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self._run_single_backtest, config, strategy_params): config
                    for config in configs
                }
                
                completed = 0
                for future in as_completed(futures):
                    completed += 1
                    result = future.result()
                    self.results.append(result)
                    
                    if self.progress_callback:
                        self.progress_callback(completed, total, result)
                    
                    if completed % 10 == 0:
                        elapsed = time.time() - start_time
                        avg_time = elapsed / completed
                        eta = avg_time * (total - completed)
                        logger.info(f"â³ è¿›åº¦ï¼š{completed}/{total} | é¢„è®¡å‰©ä½™ï¼š{eta:.0f}ç§’")
        else:
            # å¤šçº¿ç¨‹æ¨¡å¼ (é€‚åˆ I/O å¯†é›†å‹)
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self._run_single_backtest, config, strategy_params): config
                    for config in configs
                }
                
                completed = 0
                for future in as_completed(futures):
                    completed += 1
                    result = future.result()
                    self.results.append(result)
                    
                    if self.progress_callback:
                        self.progress_callback(completed, total, result)
                    
                    if completed % 10 == 0:
                        elapsed = time.time() - start_time
                        avg_time = elapsed / completed
                        eta = avg_time * (total - completed)
                        logger.info(f"â³ è¿›åº¦ï¼š{completed}/{total} | é¢„è®¡å‰©ä½™ï¼š{eta:.0f}ç§’")
        
        elapsed = time.time() - start_time
        successful = sum(1 for r in self.results if r.status == 'success')
        
        logger.info(f"âœ… æ‰¹é‡å›æµ‹å®Œæˆï¼š{successful}/{total} æˆåŠŸ | è€—æ—¶ï¼š{elapsed:.1f}ç§’")
        
        return self.results
    
    def save_results(self, output_dir: str, prefix: str = "batch"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = output_path / f"{prefix}_detailed_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(
                [r.to_dict() for r in self.results],
                f,
                indent=2,
                ensure_ascii=False,
                default=str
            )
        
        # ä¿å­˜æ±‡æ€»
        summary = self.generate_summary()
        summary_file = output_path / f"{prefix}_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜ CSV æ ¼å¼
        summary_df = self.results_to_dataframe()
        csv_file = output_path / f"{prefix}_summary_{timestamp}.csv"
        summary_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"ğŸ“ ç»“æœå·²ä¿å­˜ï¼š{output_path}")
        logger.info(f"   - è¯¦ç»†ç»“æœï¼š{detailed_file.name}")
        logger.info(f"   - æ±‡æ€»ï¼š{summary_file.name}")
        logger.info(f"   - CSV: {csv_file.name}")
        
        return str(output_path)
    
    def generate_summary(self) -> Dict:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        successful = [r for r in self.results if r.status == 'success']
        
        if not successful:
            return {'error': 'æ— æˆåŠŸå›æµ‹ç»“æœ'}
        
        # ç»©æ•ˆç»Ÿè®¡
        total_returns = [r.metrics.total_return for r in successful if r.metrics]
        sharpe_ratios = [r.metrics.sharpe_ratio for r in successful if r.metrics]
        max_drawdowns = [r.metrics.max_drawdown for r in successful if r.metrics]
        
        summary = {
            'total_symbols': len(self.results),
            'successful': len(successful),
            'failed': len(self.results) - len(successful),
            'skipped': sum(1 for r in self.results if r.status == 'skipped'),
            'total_duration_seconds': sum(r.duration_seconds for r in self.results),
            'performance': {
                'avg_return': np.mean(total_returns) if total_returns else 0,
                'median_return': np.median(total_returns) if total_returns else 0,
                'best_return': max(total_returns) if total_returns else 0,
                'worst_return': min(total_returns) if total_returns else 0,
                'avg_sharpe': np.mean(sharpe_ratios) if sharpe_ratios else 0,
                'avg_max_drawdown': np.mean(max_drawdowns) if max_drawdowns else 0,
            },
            'top_performers': [],
            'worst_performers': []
        }
        
        # TOP  performers
        sorted_by_return = sorted(
            [r for r in successful if r.metrics],
            key=lambda x: x.metrics.total_return,
            reverse=True
        )
        
        for r in sorted_by_return[:5]:
            summary['top_performers'].append({
                'symbol': r.symbol,
                'name': r.name,
                'total_return': r.metrics.total_return,
                'sharpe_ratio': r.metrics.sharpe_ratio,
                'max_drawdown': r.metrics.max_drawdown
            })
        
        # Worst performers
        for r in sorted_by_return[-5:]:
            summary['worst_performers'].append({
                'symbol': r.symbol,
                'name': r.name,
                'total_return': r.metrics.total_return,
                'sharpe_ratio': r.metrics.sharpe_ratio,
                'max_drawdown': r.metrics.max_drawdown
            })
        
        return summary
    
    def results_to_dataframe(self) -> pd.DataFrame:
        """å°†ç»“æœè½¬æ¢ä¸º DataFrame"""
        data = []
        
        for r in self.results:
            row = {
                'symbol': r.symbol,
                'name': r.name,
                'status': r.status,
                'total_return': r.metrics.total_return if r.metrics else None,
                'annual_return': r.metrics.annual_return if r.metrics else None,
                'sharpe_ratio': r.metrics.sharpe_ratio if r.metrics else None,
                'max_drawdown': r.metrics.max_drawdown if r.metrics else None,
                'volatility': r.metrics.volatility if r.metrics else None,
                'total_trades': r.metrics.total_trades if r.metrics else None,
                'win_rate': r.metrics.win_rate if r.metrics else None,
                'profit_factor': r.metrics.profit_factor if r.metrics else None,
                'duration_seconds': r.duration_seconds,
                'cache_hit': r.cache_hit,
                'error_message': r.error_message
            }
            data.append(row)
        
        return pd.DataFrame(data)


# ä¾¿æ·å‡½æ•°
def create_etf_config(
    symbol: str,
    name: str = "",
    start_date: str = "2024-01-01",
    end_date: str = "2026-12-31",
    initial_capital: float = 100000.0
) -> BacktestConfig:
    """åˆ›å»º ETF å›æµ‹é…ç½®"""
    return BacktestConfig(
        symbol=symbol,
        name=name,
        start_date=start_date,
        end_date=end_date,
        initial_capital=initial_capital,
        commission_rate=0.0003,  # ETF ä½£é‡‘è¾ƒä½
        slippage_model="fixed",
        slippage_params={"slippage_per_share": 0.001},  # ETF æ»‘ç‚¹è¾ƒå°
        impact_model="sqrt",
        impact_params={"impact_factor": 0.05}  # ETF å†²å‡»æˆæœ¬è¾ƒä½
    )


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("æ‰¹é‡å›æµ‹å¼•æ“å·²åŠ è½½")
    print(f"ç‰ˆæœ¬ï¼š2026-03-01")
    print(f"ä½œè€…ï¼šBacker-Agent (Q è„‘å›æµ‹æ¶æ„å¸ˆ)")
